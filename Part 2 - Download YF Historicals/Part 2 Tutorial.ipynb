{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YF_FullHistroicalsDownloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0txAZVZD5xRXqdYm2sjem",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muchcreative/15Yr-Free-Historical-Data-and-Cleaning/blob/main/YF_FullHistroicalsDownloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj6aFRrEkc7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7f7504-f35e-4a44-af5a-df1771f01b21"
      },
      "source": [
        "'''\n",
        "  - Download all applicable tickers from YF\n",
        "  - Compare with IEX and store data\n",
        "  - Considered HDF5 / CSV / Pickle Format\n",
        "  - Pickle format is more for a temporary storage\n",
        "  - If you are consistently updating the historicals, I recommend CSV or HDF5\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%pip install yfinance #Install modules as needed, if you using an ide use the python magic function \"%pip install\" to always get the latest version\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from pathlib import Path\n",
        "import json\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/April')\n",
        "\n",
        "from pipelines import loaders, downloaders, url_generator\n",
        "print(\"Imported Libraries\") "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.69)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.10)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Imported Libraries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwIeixyhiEni"
      },
      "source": [
        "iex_sp500_constituents_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/iexS&P500constituents.json'\n",
        "missing_iex_constituents_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/missingiexconstituents.json'\n",
        "\n",
        "with open(iex_sp500_constituents_filepath, 'r') as f:\n",
        "  iex_sp500_constituents = json.load(f)\n",
        "\n",
        "with open(missing_iex_constituents_filepath, 'r') as f:\n",
        "  missing_iex_constituents = json.load(f)\n",
        "\n",
        "market_tickers = ['SPY','DIA','QQQ','^VIX']\n",
        "\n",
        "tickers = iex_sp500_constituents + missing_iex_constituents + market_tickers\n",
        "start_date = '2017-01-01'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check len of yf_tickers\n",
        "print('We need to download {} tickers'.format(len(tickers)))\n",
        "\n",
        "#Slice the tickers to make downloading easier and upload historicals to files in parts\n",
        "sliced_tickers = tickers[:10]\n",
        "print(sliced_tickers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrv2LVXZbC8c",
        "outputId": "45266043-15d8-43f7-dbd2-220c13a79422"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We need to download 848 tickers\n",
            "['A', 'AAL', 'AAP', 'AAPL', 'ABBV', 'ABC', 'ABMD', 'ABT', 'ACN', 'ADBE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_yf_tickers(tickers, period='15y'): #You can take more than 15yrs here or specifiy a start_date and end_date\n",
        "  historicals = dict()\n",
        "  tickers_avaliable_on_yf = []\n",
        "  tickers_not_avaliable_on_yf = []\n",
        "\n",
        "  for ticker in tickers:\n",
        "    ticker_ref = yf.Ticker(ticker)\n",
        "    ticker_history = ticker_ref.history(period=period, \n",
        "                                        auto_adjust=True) #auto_adjust=True, give adjusted OHLC\n",
        "    if ticker_history.empty: #Returns an empty DataFrame if YF history doesn't exist\n",
        "      tickers_not_avaliable_on_yf.append(ticker)\n",
        "    else: \n",
        "      historicals[ticker] = ticker_history\n",
        "      tickers_avaliable_on_yf.append(ticker)\n",
        "  return (historicals, tickers_avaliable_on_yf, tickers_not_avaliable_on_yf)\n",
        "\n",
        "historicals, tickers_avaliable_on_yf, tickers_not_avaliable_on_yf = download_yf_tickers(sliced_tickers)"
      ],
      "metadata": {
        "id": "IQ_YjgnwycMc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep track of the tickers that were avaliable on yf and those that weren't\n",
        "def record_attendance_of_tickers_on_yf(tickers_avaliable_on_yf,\n",
        "                                       tickers_not_avaliable_on_yf,\n",
        "                                       logs_filepath):\n",
        "  record_avaliable_tickers_to_json(tickers_avaliable_on_yf, logs_filepath) #Both record functions use same code, but differentiated for easier readability\n",
        "  record_missing_tickers_to_json(tickers_not_avaliable_on_yf, logs_filepath)\n",
        "  print('Attendance Taken and Recorded to Jsons Respectively')\n",
        "  return\n",
        "\n",
        "def record_avaliable_tickers_to_json(tickers_avaliable_on_yf, filepath):\n",
        "  avaliable_tickers_log_filepath = f'{filepath}/avaliable_yf_tickers.json'\n",
        "  avaliable_tickers_file = Path(avaliable_tickers_log_filepath) #Call full list missing tickers file\n",
        "\n",
        "  if avaliable_tickers_file.is_file(): #Check if file already exists, if so extend and overwrite list\n",
        "    with open(avaliable_tickers_log_filepath, 'r+', encoding='utf-8') as f:\n",
        "      avaliable_tickers = json.load(f)\n",
        "      avaliable_tickers.extend(tickers_avaliable_on_yf)\n",
        "      f.seek(0)\n",
        "      json.dump(avaliable_tickers, f, ensure_ascii=False, indent=4)\n",
        "  else: #If file does not exist, create it and dump current missing tickers list into it\n",
        "    with open(avaliable_tickers_log_filepath , 'w', encoding='utf-8') as f:\n",
        "      json.dump(tickers_avaliable_on_yf, f, ensure_ascii=False, indent=4)\n",
        "  print('Avaliable tickers have been logged to avaliable tickers list')\n",
        "  return\n",
        "\n",
        "def record_missing_tickers_to_json(tickers_not_avaliable_on_yf, filepath):\n",
        "  missing_tickers_log_filepath = f'{filepath}/missing_yf_tickers.json'\n",
        "  missing_tickers_file = Path( missing_tickers_log_filepath) #Call full list missing tickers file\n",
        "\n",
        "  if missing_tickers_file.is_file(): #Check if file already exists, if so extend and overwrite list\n",
        "    with open(missing_tickers_log_filepath, 'r+', encoding='utf-8') as f:\n",
        "      missing_tickers = json.load(f)\n",
        "      missing_tickers.extend(tickers_not_avaliable_on_yf)\n",
        "      f.seek(0)\n",
        "      json.dump(missing_tickers, f, ensure_ascii=False, indent=4)\n",
        "  else: #If file does not exist, create it and dump current missing tickers list into it\n",
        "    with open(missing_tickers_log_filepath, 'w', encoding='utf-8') as f:\n",
        "      json.dump(tickers_not_avaliable_on_yf, f, ensure_ascii=False, indent=4)\n",
        "  print('Missing tickers have been logged to missing tickers list')\n",
        "  return\n",
        "\n",
        "logs_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/yf/logs'\n",
        "record_attendance_of_tickers_on_yf(tickers_avaliable_on_yf,\n",
        "                                       tickers_not_avaliable_on_yf,\n",
        "                                       logs_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTudJ3dwYqqS",
        "outputId": "26ec9442-9e41-41bf-87e5-e1458276f7c0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliable tickers have been logged to avaliable tickers list\n",
            "Missing tickers have been logged to missing tickers list\n",
            "Attendance Taken and Recorded to Jsons Respectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_historicals_to_csv(historicals):\n",
        "  '''\n",
        "  Description:\n",
        "    - Remove dividends and stock splits, as adjusted OHLC will already consider them\n",
        "  Returns:\n",
        "    - Formatted historicals for a CSV file\n",
        "  '''\n",
        "  for ticker in historicals:\n",
        "    historicals[ticker] = historicals[ticker].drop(['Dividends', 'Stock Splits'], axis='columns')\n",
        "#    historicals[ticker] = historicals[ticker].reset_index()\n",
        "  return historicals\n",
        "\n",
        "def format_historicals_to_hdf5(historicals):\n",
        "  '''\n",
        "  Description:\n",
        "    - Remove dividends and stock splits, as adjusted OHLC will already consider them\n",
        "    - Change datetime to timestamps for HDF5 as HDF5 does not accept datetimes\n",
        "  Returns:\n",
        "    - Formatted historicals for an HDF5 file\n",
        "  '''\n",
        "  for ticker in historicals:\n",
        "    historicals[ticker] = historicals[ticker].drop(['Dividends', 'Stock Splits'], axis='columns')\n",
        "    historicals[ticker] = historicals[ticker].reset_index()\n",
        "    historicals[ticker]['Date'] = historicals[ticker]['Date'].apply(lambda x: x.timestamp())\n",
        "  return historicals\n",
        "\n",
        "#historicals = format_historicals_to_csv(historicals)\n",
        "historicals = format_historicals_to_hdf5(historicals)"
      ],
      "metadata": {
        "id": "vzGIFqTuW0IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save historicals to HDF5\n",
        "historicals_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/yf'\n",
        "\n",
        "#If you prefer CSV files\n",
        "def save_historicals_to_csv(historicals, filepath):\n",
        "  for ticker in historicals:\n",
        "    csv_filepath = f'{filepath}/{ticker}.csv'\n",
        "    historicals[ticker].to_csv(csv_filepath)\n",
        "    print('Ticker {} Saved to CSV'.format(ticker))\n",
        "  print('All Tickers Have Been Saved')\n",
        "\n",
        "#If you prefer HDF5 files, you will need to use a formatter\n",
        "#Alternatively you can use pd.hdfstore but I prefer to have it as saved as a numpy array\n",
        "def save_historicals_to_hdf5(historicals, filepath):\n",
        "  for ticker in historicals:\n",
        "    hdf5_filepath = f'{filepath}/{ticker}.hdf5'\n",
        "    with h5py.File(hdf5_filepath, 'w') as f:\n",
        "      history = f.create_group('historicals')\n",
        "      history.create_dataset(name='15Y', data=historicals[ticker], compression='gzip')\n",
        "    print('Saved {} as HDF5'.format(ticker))\n",
        "\n",
        "#save_historicals_to_csv(historicals, historicals_filepath)\n",
        "save_historicals_to_hdf5(historicals, historicals_filepath)"
      ],
      "metadata": {
        "id": "rBv2YmScVD6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d482c6d-1557-4f72-985a-1db3e45f1f9e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved ^VIX as HDF5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_if_tickers_were_saved_successfully(tickers_avaliable_on_yf, \n",
        "                                        filepath,\n",
        "                                        save_type='csv'):\n",
        "  assert save_type in ['csv', 'hdf5'], 'Save type must be \"csv\" or \"hdf5\"'\n",
        "\n",
        "  tickers_not_saved = []\n",
        "  for ticker in tickers_avaliable_on_yf:\n",
        "    ticker_filepath = f'{filepath}/{ticker}.{save_type}'\n",
        "    ticker_file = Path(ticker_filepath)\n",
        "    if ticker_file.is_file():\n",
        "      pass\n",
        "    else:\n",
        "      print(\"{} is missing\".format(ticker))\n",
        "      tickers_not_saved.append(ticker)\n",
        "  return tickers_not_saved\n",
        "\n",
        "tickers_not_saved = check_if_tickers_saved_successfully(tickers_avaliable_on_yf, \n",
        "                                                        historicals_filepath,\n",
        "                                                        save_type='hdf5') #Change to csv or hdf5 depending on the format you used\n",
        "print('Tickers NOT saved successfully were {}'.format(tickers_not_saved))"
      ],
      "metadata": {
        "id": "8VyfjGzlaNgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff0a387-052d-48af-fc7f-d6e5a36c4f17"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tickers NOT saved successfully were []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For reference on loading the database\n",
        "def load_csv_tickers_as_pd_historicals(tickers, filepath):\n",
        "  historicals = dict()\n",
        "  '''\n",
        "    Description:\n",
        "      - Takes tickers as a list ['A', 'AAPL', 'AMZN']\n",
        "      - Formats CSV with pandas \"pd.read_csv\"\n",
        "    Returns:\n",
        "      - Historicals dict() containing formatted pandas DataFrames with tickers as keys\n",
        "  '''\n",
        "  for ticker in tickers:\n",
        "    csv_filepath = f'{filepath}/{ticker}.csv'\n",
        "    dataset = pd.read_csv(csv_filepath, index_col='Date')\n",
        "    historicals[ticker] = dataset\n",
        "  return historicals\n",
        "\n",
        "def load_hdf5_tickers_as_pd_historicals(tickers, filepath):\n",
        "  historicals = dict()\n",
        "  columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
        "  \n",
        "  for ticker in tickers:\n",
        "    hdf5_filepath = f'{filepath}/{ticker}.hdf5'\n",
        "    with h5py.File(hdf5_filepath, 'r') as f:\n",
        "      group = f['historicals']\n",
        "      data = group['15Y'][()]\n",
        "      \n",
        "    dataset = pd.DataFrame(data=data, columns=columns)\n",
        "    dataset['Date'] = pd.to_datetime(dataset['Date'], unit='s')\n",
        "    dataset = dataset.set_index('Date')\n",
        "    historicals[ticker] = dataset\n",
        "  return historicals\n",
        "\n",
        "#historicals = load_csv_tickers_as_pd_historicals(['A'], historicals_filepath)\n",
        "historicals = load_hdf5_tickers_as_pd_historicals(['A'], historicals_filepath)"
      ],
      "metadata": {
        "id": "PCgJggrq4Bf2"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}
