{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 3A Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJa2iqhfqSss",
        "outputId": "17fbf2ed-d0ef-4280-94cf-b30a1306278f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Importing Complete\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import h5py\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/April/')\n",
        "\n",
        "print(\"Importing Complete\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's take a look at how many tickers were on yahoo finance and those that were missing\n",
        "#We will also need all the SP500 changes to check if our historicals match up with the time they were in the SP500\n",
        "\n",
        "avaliable_yf_tickers_filepath = '/p3data/logs/avaliable_yf_tickers.json'\n",
        "avaliable_yf_tickers_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/yf/logs/avaliable_yf_tickers.json'\n",
        "\n",
        "missing_yf_tickers_filepath = '/p3data/logs/missing_yf_tickers.json'\n",
        "missing_yf_tickers_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/yf/logs/missing_yf_tickers.json'\n",
        "\n",
        "sp500_changes_filepath = 'p3data/S&P500 Consitutents 20061009-20220116.json'\n",
        "sp500_changes_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/S&P500 Consitutents 20061009-20220116.json'\n",
        "\n",
        "with open(avaliable_yf_tickers_filepath, 'r') as f:\n",
        "  avaliable_yf_tickers = json.load(f)\n",
        "\n",
        "with open(missing_yf_tickers_filepath, 'r') as f:\n",
        "  missing_yf_tickers = json.load(f)\n",
        "\n",
        "sp500_changes = pd.read_json(sp500_changes_filepath) #Reminder that we saved the sp500_changes with pd.to_json in part 2 so we can load with pd.read_json() here\n",
        "\n",
        "print('You have {} avaliable tickers and {} missing tickers for a total of {} tickers'.format(len(avaliable_yf_tickers),\n",
        "                                                                                              len(missing_yf_tickers),\n",
        "                                                                                              len(avaliable_yf_tickers+missing_yf_tickers)))"
      ],
      "metadata": {
        "id": "cMkLDatMupee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34cb356a-19f8-49d2-e69d-001d285fcdc7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 672 avaliable tickers and 177 missing tickers for a total of 849 tickers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reminder to handle this\n",
        "#avaliable_yf_tickers.remove('PGL')\n",
        "#avaliable_yf_tickers.remove('UVN')"
      ],
      "metadata": {
        "id": "R5-X_DRL7fpS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's also load the historicals we recently downloaded from Yahoo Finance to memory\n",
        "#I am loading them from hdf5 files,  but the csv loader is avaliable at the end of tutorial 2\n",
        "\n",
        "def load_hdf5_tickers_as_pd_historicals(tickers, filepath):\n",
        "  historicals = dict()\n",
        "  columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
        "  \n",
        "  for ticker in tickers:\n",
        "    hdf5_filepath = f'{filepath}/{ticker}.hdf5'\n",
        "    ticker_file = Path(hdf5_filepath)\n",
        "    if ticker_file.is_file():\n",
        "      with h5py.File(hdf5_filepath, 'r') as f:\n",
        "        group = f['historicals']\n",
        "        data = group['15Y'][()]\n",
        "      dataset = pd.DataFrame(data=data, columns=columns)\n",
        "      dataset['Date'] = pd.to_datetime(dataset['Date'], unit='s') #Change our timestamps back to datetimes\n",
        "      dataset = dataset.set_index('Date')\n",
        "      historicals[ticker] = dataset\n",
        "    else:\n",
        "      print('Error {} ticker is missing'.format(ticker))\n",
        "  print('All Historicals Have Been Saved to Memory')\n",
        "  return historicals\n",
        "\n",
        "historicals_filepath = 'p3data/historicals'\n",
        "historicals_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/yf'\n",
        "historicals = load_hdf5_tickers_as_pd_historicals(avaliable_yf_tickers, historicals_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdUhKwWHkTzU",
        "outputId": "baa5ebb1-4dcd-4623-9bd9-c3cc60585341"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error PGL ticker is missing\n",
            "Error UVN ticker is missing\n",
            "All Historicals Have Been Saved to Memory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's look at the data we have\n",
        "\n",
        "def count_all_historicals_lengths(historicals):\n",
        "  hist_len_counter = dict()\n",
        "  for ticker, historical in historicals.items():\n",
        "    hist_len_counter.setdefault(len(historical), 0)\n",
        "    hist_len_counter[len(historical)] += 1\n",
        "  return hist_len_counter\n",
        "\n",
        "hist_len_counter = count_all_historicals_lengths(historicals)\n",
        "\n",
        "#Give them a quick sort by most common lengths and highest lenths\n",
        "most_common_lens = sorted(hist_len_counter.items(), key=lambda x:x[1], reverse=True)\n",
        "highest_lens = sorted(hist_len_counter.items(), key=lambda x:x[0], reverse=True)\n",
        "\n",
        "print('Most Common Lens:\\n {}'.format(most_common_lens))\n",
        "print('\\nHighest Lens:\\n {}'.format(highest_lens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JObkXOBlV4jr",
        "outputId": "c8a5e5de-46b3-45d6-93d7-bce173f5690e"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Common Lens:\n",
            " [(3777, 515), (464, 2), (1882, 2), (3677, 2), (3778, 2), (1649, 2), (1954, 2), (2163, 2), (2279, 1), (423, 1), (1008, 1), (2988, 1), (2057, 1), (1061, 1), (2437, 1), (2945, 1), (1920, 1), (2559, 1), (455, 1), (3136, 1), (3461, 1), (493, 1), (1137, 1), (3118, 1), (2734, 1), (3735, 1), (2367, 1), (3161, 1), (2978, 1), (2760, 1), (1149, 1), (2921, 1), (941, 1), (2157, 1), (1844, 1), (238, 1), (3032, 1), (2809, 1), (2080, 1), (2167, 1), (2540, 1), (1154, 1), (598, 1), (670, 1), (138, 1), (3696, 1), (1846, 1), (3706, 1), (1366, 1), (3066, 1), (3358, 1), (392, 1), (716, 1), (338, 1), (2468, 1), (2504, 1), (823, 1), (3002, 1), (1704, 1), (1299, 1), (2332, 1), (2434, 1), (2603, 1), (1559, 1), (2793, 1), (721, 1), (722, 1), (2797, 1), (3063, 1), (1397, 1), (2811, 1), (3006, 1), (3072, 1), (2735, 1), (28, 1), (529, 1), (2727, 1), (2040, 1), (502, 1), (840, 1), (1575, 1), (1313, 1), (1911, 1), (2191, 1), (1181, 1), (73, 1), (3451, 1), (1826, 1), (2753, 1), (3754, 1), (1689, 1), (2235, 1), (1306, 1), (2954, 1), (3745, 1), (2069, 1), (215, 1), (2661, 1), (785, 1), (3569, 1), (2312, 1), (2267, 1), (156, 1), (3037, 1), (2764, 1), (2405, 1), (1610, 1), (2884, 1), (173, 1), (1956, 1), (700, 1), (1020, 1), (3487, 1), (2460, 1), (62, 1), (1775, 1), (2298, 1), (141, 1), (756, 1), (2780, 1), (872, 1), (1069, 1), (1718, 1), (243, 1), (3000, 1), (2691, 1), (2451, 1), (2777, 1), (2348, 1), (2960, 1), (3602, 1), (2534, 1), (3716, 1), (2745, 1), (2546, 1), (2911, 1), (2064, 1), (2871, 1), (1468, 1), (3584, 1), (3013, 1), (3485, 1), (332, 1), (3093, 1), (3776, 1), (1656, 1), (601, 1), (2584, 1), (2258, 1)]\n",
            "\n",
            "Highest Lens:\n",
            " [(3778, 2), (3777, 515), (3776, 1), (3754, 1), (3745, 1), (3735, 1), (3716, 1), (3706, 1), (3696, 1), (3677, 2), (3602, 1), (3584, 1), (3569, 1), (3487, 1), (3485, 1), (3461, 1), (3451, 1), (3358, 1), (3161, 1), (3136, 1), (3118, 1), (3093, 1), (3072, 1), (3066, 1), (3063, 1), (3037, 1), (3032, 1), (3013, 1), (3006, 1), (3002, 1), (3000, 1), (2988, 1), (2978, 1), (2960, 1), (2954, 1), (2945, 1), (2921, 1), (2911, 1), (2884, 1), (2871, 1), (2811, 1), (2809, 1), (2797, 1), (2793, 1), (2780, 1), (2777, 1), (2764, 1), (2760, 1), (2753, 1), (2745, 1), (2735, 1), (2734, 1), (2727, 1), (2691, 1), (2661, 1), (2603, 1), (2584, 1), (2559, 1), (2546, 1), (2540, 1), (2534, 1), (2504, 1), (2468, 1), (2460, 1), (2451, 1), (2437, 1), (2434, 1), (2405, 1), (2367, 1), (2348, 1), (2332, 1), (2312, 1), (2298, 1), (2279, 1), (2267, 1), (2258, 1), (2235, 1), (2191, 1), (2167, 1), (2163, 2), (2157, 1), (2080, 1), (2069, 1), (2064, 1), (2057, 1), (2040, 1), (1956, 1), (1954, 2), (1920, 1), (1911, 1), (1882, 2), (1846, 1), (1844, 1), (1826, 1), (1775, 1), (1718, 1), (1704, 1), (1689, 1), (1656, 1), (1649, 2), (1610, 1), (1575, 1), (1559, 1), (1468, 1), (1397, 1), (1366, 1), (1313, 1), (1306, 1), (1299, 1), (1181, 1), (1154, 1), (1149, 1), (1137, 1), (1069, 1), (1061, 1), (1020, 1), (1008, 1), (941, 1), (872, 1), (840, 1), (823, 1), (785, 1), (756, 1), (722, 1), (721, 1), (716, 1), (700, 1), (670, 1), (601, 1), (598, 1), (529, 1), (502, 1), (493, 1), (464, 2), (455, 1), (423, 1), (392, 1), (338, 1), (332, 1), (243, 1), (238, 1), (215, 1), (173, 1), (156, 1), (141, 1), (138, 1), (73, 1), (62, 1), (28, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#From above we see that the highest lens is 3778 while the majority of the historicals (515 of them) sit at a comfortable 3777 length\n",
        "#Additionally in \"Highest Lens\", we see that past the length of 3777, very few tickers have the same amount of missing tickers\n",
        "#Meaning that 3777 is the typical length of 15 years of data and we will use this length for reference and comparison\n",
        "#However, the 3778 length does stand out, let's take a look at which tickers are causing this\n",
        "\n",
        "def look_for_target_historicals_lengths(historicals, target_len):\n",
        "  target_len_tickers = [ticker\n",
        "                        for ticker, historical in historicals.items()\n",
        "                        if target_len == len(historical)]\n",
        "  return target_len_tickers\n",
        "\n",
        "outlier_len = 3778\n",
        "outlier_len_tickers = look_for_target_historicals_lengths(historicals, outlier_len)\n",
        "print(outlier_len_tickers) #We see that GS stands out. Since this is a single ticker, we will simply cut the data point later that is extra from the dataset to keep the data uniform\n",
        "\n",
        "standard_len = 3777\n",
        "standard_len_tickers = look_for_target_historicals_lengths(historicals, standard_len)\n",
        "print(standard_len_tickers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs-IUWeVnIk6",
        "outputId": "479201ce-c4e0-4cf4-9ae8-89882749b8f9"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DIA', 'GS']\n",
            "['A', 'AAL', 'AAP', 'AAPL', 'ABC', 'ABMD', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADS', 'ADSK', 'AEE', 'AEP', 'AES', 'AFL', 'AIG', 'AIV', 'AIZ', 'AJG', 'AKAM', 'ALB', 'ALGN', 'ALK', 'ALL', 'AMAT', 'AMD', 'AME', 'AMG', 'AMGN', 'AMP', 'AMT', 'AMZN', 'AN', 'ANF', 'ANSS', 'ANTM', 'AON', 'AOS', 'APA', 'APD', 'APH', 'ARE', 'ASH', 'ATGE', 'ATI', 'ATO', 'ATVI', 'AVB', 'AVY', 'AXP', 'AYI', 'AZO', 'BA', 'BAC', 'BAX', 'BBBY', 'BBWI', 'BBY', 'BC', 'BDX', 'BEN', 'BIG', 'BIIB', 'BIO', 'BK', 'BKNG', 'BKR', 'BLK', 'BLL', 'BMY', 'BRO', 'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CAT', 'CB', 'CBRE', 'CCEP', 'CCI', 'CCL', 'CCU', 'CDNS', 'CE', 'CERN', 'CF', 'CHD', 'CHRW', 'CI', 'CIEN', 'CINF', 'CL', 'CLF', 'CLX', 'CMA', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'CNX', 'COF', 'COO', 'COP', 'COST', 'CPB', 'CPRT', 'CRL', 'CRM', 'CSCO', 'CSX', 'CTAS', 'CTRA', 'CTSH', 'CTXS', 'CVS', 'CVX', 'D', 'DD', 'DDS', 'DE', 'DGX', 'DHI', 'DHR', 'DIS', 'DISCA', 'DISH', 'DLR', 'DLTR', 'DOV', 'DPZ', 'DRE', 'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DXC', 'DXCM', 'EA', 'EBAY', 'ECL', 'ED', 'EFX', 'EIX', 'EL', 'EMN', 'EMR', 'ENDP', 'EOG', 'EQIX', 'EQR', 'EQT', 'ES', 'ESS', 'ETN', 'ETR', 'EVRG', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FAST', 'FCX', 'FDS', 'FDX', 'FE', 'FFIV', 'FHN', 'FIS', 'FISV', 'FITB', 'FL', 'FLR', 'FLS', 'FMC', 'FMCC', 'FNMA', 'FOSL', 'FRT', 'FSLR', 'FTI', 'GD', 'GE', 'GHC', 'GILD', 'GIS', 'GL', 'GLW', 'GME', 'GNW', 'GOOG', 'GOOGL', 'GPC', 'GPN', 'GPS', 'GRMN', 'GT', 'GWW', 'HAL', 'HAS', 'HBAN', 'HBI', 'HD', 'HES', 'HFC', 'HIG', 'HOG', 'HOLX', 'HON', 'HP', 'HPQ', 'HRB', 'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'IAC', 'IBM', 'ICE', 'IDXX', 'IEX', 'IFF', 'IGT', 'ILMN', 'INCY', 'INTC', 'INTU', 'IP', 'IPG', 'IPGP', 'IRM', 'ISRG', 'IT', 'ITT', 'ITW', 'IVZ', 'J', 'JBHT', 'JBL', 'JCI', 'JEF', 'JKHY', 'JNJ', 'JNPR', 'JPM', 'JWN', 'K', 'KBH', 'KEY', 'KIM', 'KLAC', 'KMB', 'KMX', 'KO', 'KR', 'KSS', 'L', 'LDOS', 'LEG', 'LEN', 'LH', 'LHX', 'LIN', 'LKQ', 'LLY', 'LMT', 'LNC', 'LNT', 'LOW', 'LRCX', 'LSI', 'LUMN', 'LUV', 'LVS', 'LYV', 'M', 'MA', 'MAA', 'MAC', 'MAR', 'MAS', 'MAT', 'MBI', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', 'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST', 'MO', 'MOS', 'MPWR', 'MRK', 'MRO', 'MS', 'MSFT', 'MSI', 'MTB', 'MTCH', 'MTD', 'MTG', 'MTW', 'MU', 'MUR', 'NBR', 'NCR', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NKTR', 'NLOK', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NVR', 'NWL', 'NYT', 'O', 'ODFL', 'ODP', 'OI', 'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PBI', 'PCAR', 'PCG', 'PDCO', 'PEAK', 'PEG', 'PENN', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PNC', 'PNR', 'PNW', 'POOL', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PTC', 'PVH', 'PWR', 'PXD', 'QCOM', 'QQQ', 'R', 'RCL', 'RE', 'REG', 'REGN', 'RF', 'RHI', 'RIG', 'RJF', 'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RRC', 'RRD', 'RSG', 'RTX', 'SANM', 'SBAC', 'SBNY', 'SBUX', 'SCHW', 'SEE', 'SHW', 'SIG', 'SIVB', 'SJM', 'SLB', 'SLG', 'SLM', 'SNA', 'SNPS', 'SNV', 'SO', 'SPG', 'SPGI', 'SPY', 'SRCL', 'SRE', 'SSP', 'STE', 'STT', 'STX', 'STZ', 'SWK', 'SWKS', 'SWN', 'SYK', 'SYY', 'T', 'TAP', 'TDG', 'TDY', 'TECH', 'TER', 'TEX', 'TFC', 'TFX', 'TGNA', 'TGT', 'THC', 'TJX', 'TMO', 'TPR', 'TRMB', 'TROW', 'TRV', 'TSCO', 'TSN', 'TT', 'TTWO', 'TXN', 'TXT', 'TYL', 'UAA', 'UAL', 'UDR', 'UHS', 'UIS', 'UNH', 'UNM', 'UNP', 'UPS', 'URBN', 'URI', 'USB', 'VFC', 'VIAC', 'VIAV', 'VLO', 'VMC', 'VNO', 'VRSN', 'VRTX', 'VTR', 'VTRS', 'VZ', 'WAB', 'WAT', 'WBA', 'WDC', 'WEC', 'WELL', 'WEN', 'WFC', 'WHR', 'WM', 'WMB', 'WMT', 'WRB', 'WST', 'WU', 'WY', 'WYNN', 'X', 'XEL', 'XLNX', 'XOM', 'XRAY', 'XRX', 'YUM', 'ZBH', 'ZBRA', 'ZION', '^VIX']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's make sure all the date ranges at 3777 is the same\n",
        "#From above we see that AAPL has all this data, let's just do a quick comparison that all the dates of the other tickers with the same length\n",
        "\n",
        "def check_for_uniformity_of_dates(historicals, \n",
        "                                tickers_to_compare, \n",
        "                                comparsion_ticker):\n",
        "  for ticker in tickers_to_compare:\n",
        "    if historicals[ticker].index.equals(historicals[comparison_ticker].index): \n",
        "      pass\n",
        "    else: \n",
        "      print('{} does not have the same dates as {}'.format(ticker, comparison_ticker)) #This will print only if dates do not match with AAPL\n",
        "  print('Comparison Completed')\n",
        "\n",
        "comparison_ticker = 'AAPL'\n",
        "check_for_uniformity_of_dates(historicals,\n",
        "                              standard_len_tickers,\n",
        "                              comparison_ticker)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v0EazG0qzer",
        "outputId": "f5dce749-3a7f-4246-f179-104059557923"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since we have now confirmed that all the tickers that have the length of 3777 are at the same dates, we can now get to work on the missing dates\n",
        "#Let's grab all the tickers that have missing data\n",
        "\n",
        "tickers_with_full_data = standard_len_tickers + outlier_len_tickers \n",
        "tickers_with_missing_dates = set(historicals) - set(tickers_with_full_data)\n",
        "print('Tickers with missing dates:\\n {}'.format(tickers_with_missing_dates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFJuATqxpyGj",
        "outputId": "9e14288b-bf4a-409f-9acc-e015f876449c"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tickers with missing dates:\n",
            " {'TWTR', 'EQ', 'NSM', 'NWSA', 'MDP', 'CTX', 'MRNA', 'DELL', 'ENPH', 'TRIP', 'VNT', 'SBL', 'ZTS', 'PM', 'MMI', 'WRK', 'KDP', 'ADCT', 'DISCK', 'NCC', 'CDAY', 'PLL', 'NXPI', 'UA', 'FOX', 'TSLA', 'DG', 'SEDG', 'OGN', 'WLTW', 'COL', 'CBE', 'HET', 'CPRI', 'SUN', 'CARR', 'WB', 'SII', 'VRSK', 'DAL', 'PYPL', 'XL', 'DNB', 'KSU', 'MPC', 'TWX', 'HII', 'NFX', 'DFS', 'BRL', 'SLR', 'IR', 'FCPT', 'SOV', 'LW', 'SYF', 'HCA', 'CMX', 'FB', 'ANET', 'KEYS', 'NE', 'DOW', 'NWS', 'TEK', 'HPE', 'QRVO', 'SVU', 'FOXA', 'PD', 'S', 'ALLE', 'FTNT', 'CFG', 'CHTR', 'ROH', 'IQV', 'AET', 'INFO', 'APTV', 'CSRA', 'OTIS', 'CBH', 'ETSY', 'KMI', 'BOL', 'NOW', 'HPC', 'V', 'SAF', 'BHF', 'CZR', 'BUD', 'CTLT', 'SHLD', 'LIFE', 'DYN', 'TEL', 'CDW', 'ESRX', 'NLSN', 'HWM', 'FLT', 'MSCI', 'AMCR', 'CTVA', 'FTV', 'ARNC', 'FANG', 'HLT', 'BR', 'XYL', 'KHC', 'COTY', 'PX', 'LYB', 'EPAM', 'ABBV', 'PSX', 'CA', 'SCG', 'MON', 'FRC', 'CPWR', 'GNRC', 'JAVA', 'FBHS', 'CVG', 'CBOE', 'H', 'SE', 'ULTA', 'HCP', 'EVHC', 'CHK', 'TRB', 'GM', 'SNI', 'TDC', 'TMUS', 'PAYC', 'ANDV', 'UST', 'NAVI', 'ADT', 'ALTR', 'NCLH', 'BMS', 'BEAM', 'HOT', 'AVGO', 'LLL', 'AWK'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile all the missing dates for each ticker with missing dates we will compare it the full date range that the other tickers have\n",
        "def compile_all_missing_dates(historicals,\n",
        "                              tickers_with_missing_dates,\n",
        "                              full_date_range):\n",
        "  missing_tickers_and_dates = {ticker: full_date_range.difference(historicals[ticker].index) \n",
        "                              for ticker in tickers_with_missing_dates}\n",
        "  return missing_tickers_and_dates\n",
        "\n",
        "full_date_range = historicals['AAPL'].index #We already know AAPL has the entire date range, let's get its dates for comparsion\n",
        "\n",
        "missing_tickers_and_dates = compile_all_missing_dates(historicals,\n",
        "                                                      tickers_with_missing_dates,\n",
        "                                                      full_date_range)\n",
        "\n",
        "print(missing_tickers_and_dates['TWTR']) #Let's double check and take a look at one of our tickers and the dates its missing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgKi_D8q6Mg-",
        "outputId": "f48c6b24-656d-4af7-a58f-357bbb2253f3"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatetimeIndex(['2007-01-22', '2007-01-23', '2007-01-24', '2007-01-25',\n",
            "               '2007-01-26', '2007-01-29', '2007-01-30', '2007-01-31',\n",
            "               '2007-02-01', '2007-02-02',\n",
            "               ...\n",
            "               '2013-10-24', '2013-10-25', '2013-10-28', '2013-10-29',\n",
            "               '2013-10-30', '2013-10-31', '2013-11-01', '2013-11-04',\n",
            "               '2013-11-05', '2013-11-06'],\n",
            "              dtype='datetime64[ns]', name='Date', length=1713, freq=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since we are only looking for the dates when they are in the SP500\n",
        "#We can filter out the missing dates when they are not in the SP500 as we will not be using those\n",
        "#We will need the sp500 changes we loaded in earlier\n",
        "#Additionally we will have to consider if the ticker drops out from the SP500 and re-enters at a later date\n",
        "#This will require some legwork to get\n",
        "\n",
        "#Find first date, can try aggrgate\n",
        "#Then ensure that each next date\n",
        "#Change entire column to a chained list, then use iloc LOL, which is not a bad idea\n",
        "#Take a slice\n",
        "\n",
        "def filter_out_when_dates_not_in_sp500(missing_tickers_and_dates, sp500_changes):\n",
        "  \n",
        "  for ticker in missing_tickers_and_dates:\n",
        "\n",
        "\n",
        "  if ticker in sp500_changes.loc[date,tickers]:\n",
        "\n",
        "\n",
        "  pass\n",
        "\n",
        "#All dates it is in the sp500\n",
        "#Then compare it \n",
        "\n",
        "#Using start and end dates and knowing what data we are missing, trim the missing data list to only include the time the ticker was in the sp500\n",
        "def _trim_dates_outside_of_sp500_date_range(missing_historicals, date_range_for_tickers_in_sp500):\n",
        "  for ticker, (start_date_in_sp500, end_date_in_sp500) in date_range_for_tickers_in_sp500.items():\n",
        "    for date in missing_historicals[ticker]:\n",
        "      if date < start_date_in_sp500 or date > end_date_in_sp500:\n",
        "        missing_historicals[ticker].remove(date)\n",
        "        if missing_historicals[ticker] == []:\n",
        "          missing_historicals.pop(ticker, None)\n",
        "  return missing_historicals\n",
        "\n",
        "#Find Start and End Date for the missing tickers for when they were in the SP500\n",
        "def _find_when_missing_ticker_was_in_sp500(missing_historicals, changes_in_sp500):\n",
        "  date_range_for_tickers_in_sp500 = {}\n",
        "  for ticker in missing_historicals:\n",
        "    date_range_for_tickers_in_sp500[ticker] = [_find_ticker_start_date_in_sp500(ticker, changes_in_sp500), \n",
        "                          _find_ticker_end_date_in_sp500(ticker, changes_in_sp500)]\n",
        "  return date_range_for_tickers_in_sp500\n",
        "\n",
        "def _find_ticker_start_date_in_sp500(ticker, sp500_changes):\n",
        "  for date, sp500_constituents in sp500_changes.itertuples():\n",
        "    if ticker in sp500_constituents:\n",
        "      start_date = date\n",
        "      return start_date\n",
        "      \n",
        "def _find_ticker_end_date_in_sp500(ticker, sp500_changes):\n",
        "  for date, sp500_constituents in sp500_changes.iloc[::-1].itertuples():\n",
        "    if ticker in sp500_constituents:\n",
        "      end_date = date\n",
        "      return end_date"
      ],
      "metadata": {
        "id": "42TZOo_QC7F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's inspect the missing data and see when we are missing data using seaborn\n",
        "def count_missing_dates(missing_tickers_and_dates, full_date_range):\n",
        "  missing_dates_counter = {date: 0 for date in full_date_range}\n",
        "\n",
        "  for dates in missing_tickers_and_dates.values():\n",
        "    for date in dates:\n",
        "      missing_dates_counter[date] += 1\n",
        "  return missing_dates_counter\n",
        "\n",
        "missing_dates_and_counter = count_missing_dates(missing_tickers_and_dates, full_date_range)\n",
        "sns.barplot(data=missing_dates_and_counter)"
      ],
      "metadata": {
        "id": "TJDwRw6j9k66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check the average size of length of this data\n",
        "#Now you don't need the full length of each ticker, just the time they were in the SP500\n",
        "#Additionally, if they have history before they join the SP500, it helpful in order to run moving averages or other sliding window indicators before they join\n",
        "#But for now it is not neccssary, let's set up the bare minimum requirements and check that all the data is there\n",
        "\n",
        "#Ok we now need to keep track of these changes\n",
        "#best way too?"
      ],
      "metadata": {
        "id": "-v3JtoRw8jPN"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}