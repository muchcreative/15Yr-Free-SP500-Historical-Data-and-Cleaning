{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 3B Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1n2V_vVQ0En",
        "outputId": "cb131909-f05d-4c39-bdd1-f62993f425ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Imported Libraries\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import json\n",
        "import datetime as dt\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/April')\n",
        "\n",
        "from pipelines import loaders, downloaders, url_generator\n",
        "print(\"Imported Libraries\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Currently using updater module, perhaps should add seperate module for these type of modules and specify filepath\n",
        "#Or could set it your missing historicals\n",
        "\n",
        "iex_sp500_constituents_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/iexS&P500constituents.json'\n",
        "with open(iex_sp500_constituents_filepath, 'r') as f:\n",
        "  iex_sp500_constituents = json.load(f)\n",
        "date_length = 'max'"
      ],
      "metadata": {
        "id": "aum8aCceQ-uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Batch Urls#\n",
        "\n",
        "###Partition SP500 List to Prepare for Batch Calling and Multi Threading###\n",
        "def get_historical_batch_urls(tickers, date_length, IEX_TOKEN = IEX_TOKEN):\n",
        "  '''Returns list of partitioned batch urls for aysnc api requests'''\n",
        "  historical_batch_urls = []\n",
        "  for ticker_partition in partition(tickers):\n",
        "    ticker_partition = \",\".join(ticker_partition)\n",
        "    batch_url = f\"https://cloud.iexapis.com/stable/stock/market/batch?symbols={ticker_partition}&types=chart&range={date_length}&token={IEX_TOKEN}\"\n",
        "    historical_batch_urls.append(batch_url)\n",
        "  return historical_batch_urls\n",
        "  \n",
        "def partition(tickers, partition_size = 50):\n",
        "  partitioned_tickers = []\n",
        "  for i in range(0, len(tickers), partition_size):\n",
        "    partitioned_tickers.append(tickers[i:i+partition_size])\n",
        "  return partitioned_tickers\n",
        "\n",
        "historical_batch_urls = url_generator.get_historical_batch_urls(iex_sp500_constituents, date_length)\n",
        "print(historical_batch_urls) #Check if a reduced batch is giving you what you want\n",
        "print('We have {} batch urls that need to be downloaded'.format(len(historical_batch_urls)))"
      ],
      "metadata": {
        "id": "PMIyeBppRCOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Segregate Batch URLS for easier consumption\n",
        "#Finished [:2], [2:5], [5:8], [8:11]\n",
        "#Current [11:]\n",
        "sliced_historical_batch_urls = historical_batch_urls[11:] \n",
        "print(sliced_historical_batch_urls)"
      ],
      "metadata": {
        "id": "uyqZ_TyAREPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For IEX Historicals to HDF5#\n",
        "def get_iex_historicals(hist_batch_urls):\n",
        "  historicals = dict()\n",
        "  for hist_batch_url in hist_batch_urls:\n",
        "    try:\n",
        "      hist_response = requests.get(hist_batch_url)\n",
        "      hist_response.raise_for_status()\n",
        "      hist_response = hist_response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print('Stopped at batch url: {}'.format(hist_batch_url))\n",
        "      print('Status Code: {}'.format(hist_response.status_code))\n",
        "      raise SystemExit(e)\n",
        "    for ticker in hist_response:\n",
        "      ticker_hist = list()\n",
        "      total_amount_of_days = len(hist_response[ticker]['chart'])\n",
        "      for day in range(0, total_amount_of_days):\n",
        "        current_date = hist_response[ticker]['chart'][day]['date']\n",
        "        current_timestamp = dt.datetime.strptime(current_date,\"%Y-%m-%d\").timestamp()\n",
        "        ticker_hist.append([current_timestamp,\n",
        "                     hist_response[ticker]['chart'][day]['fopen'], #The 'f' in front of the OHLC names hash for the adjusted prices\n",
        "                     hist_response[ticker]['chart'][day]['fhigh'],\n",
        "                     hist_response[ticker]['chart'][day]['flow'],\n",
        "                     hist_response[ticker]['chart'][day]['fclose'],\n",
        "                     hist_response[ticker]['chart'][day]['fvolume']])\n",
        "      historicals[ticker] = ticker_hist\n",
        "      print('Finished downloading {}'.format(ticker))\n",
        "  return historicals\n",
        "\n",
        "#We have to test the try except request issue\n",
        "historicals = get_iex_historicals(sliced_historical_batch_urls)"
      ],
      "metadata": {
        "id": "83pA3-i1RFJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save historicals to HDF5\n",
        "def save_historicals_to_hdf5(historicals, filepath):\n",
        "  for ticker in historicals:\n",
        "    hdf5_filepath = f'{filepath}/{ticker}.hdf5'\n",
        "    with h5py.File(hdf5_filepath, 'w') as f:\n",
        "      history = f.create_group('historicals')\n",
        "      history.create_dataset(name='15Y', data=historicals[ticker], compression='gzip')\n",
        "    print('Saved {} as HDF5'.format(ticker))\n",
        "\n",
        "ml_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/ml'\n",
        "downloaders.save_historicals_to_hdf5(historicals, filepath)"
      ],
      "metadata": {
        "id": "glTJwZtHRGWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Double Check That All Has Been Downloaded\n",
        "ml_filepath = '/content/drive/MyDrive/Colab Notebooks/April/data/ml'\n",
        "\n",
        "for ticker in iex_sp500_constituents:\n",
        "  hdf5_ticker_filepath = f'{ml_filepath}/{ticker}.hdf5'\n",
        "  hdf5_file = Path(hdf5_ticker_filepath)\n",
        "  if hdf5_file.is_file():\n",
        "    pass\n",
        "  else:\n",
        "    print(\"{} is missing\".format(ticker))"
      ],
      "metadata": {
        "id": "9XyOfY5aRHdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}