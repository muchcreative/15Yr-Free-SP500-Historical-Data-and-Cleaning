{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 3B Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1n2V_vVQ0En",
        "outputId": "ab125604-9cb9-4bbe-cfbb-7ec8cf26a37c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Imported Libraries\n"
          ]
        }
      ],
      "source": [
        "#We saw that we had a lot of missing data in Part 3A, in this section we will try to remedy that with another dataset\n",
        "#This dataset comes from IEX Cloud. For 15Yr data you will need a subcription which is around 10USD per month\n",
        "#It comes with realtime data, other financial/valuation metrics, and some level of customer support\n",
        "\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from itertools import chain\n",
        "\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from p3Binputs.apitokens import IEX_TOKEN #Import your IEX_TOKEN\n",
        "                                          #If you want to try this for free you can use IEX Sandbox Mode and a Sandbox Token\n",
        "print(\"Imported Libraries\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You can decide if you want too create two full SP500 databases and average or merge your results for the most accurate OHLC\n",
        "#Or you can just download only the historicals you are missing here, here I will download only the historicals we are missing\n",
        "#But I suggest trying three databases, if you can fit it. One for YF historicals, one for IEX, and one for the merge or average results\n",
        "\n",
        "filepath = 'p3Binputs/full_missing_tickers_and_dates'\n",
        "\n",
        "with open(filepath, 'r') as f:\n",
        "  full_missing_tickers_and_dates = json.load(f)\n",
        "\n",
        "tickers = list(full_missing_tickers_and_dates.keys())\n",
        "print(tickers[:5]) #Check first 5 of the tickers we will request from IEX Cloud"
      ],
      "metadata": {
        "id": "aum8aCceQ-uq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6600024a-6152-4e2d-dc70-242bc73e8bab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AABA', 'ABI', 'ABKFQ', 'ACAS', 'ACS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You can make any changes to the tickers list here\n",
        "#For example I know the Sears tickers was spelled wrong from Part 3A, it should be $SHLDQ not SHLD\n",
        "\n",
        "tickers.remove('SHLD')\n",
        "tickers.append('SHLDQ')\n",
        "full_missing_tickers_and_dates['SHLDQ'] = full_missing_tickers_and_dates.pop('SHLD') "
      ],
      "metadata": {
        "id": "tGN0slUoBVL8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Batch Request Urls, batch urls are more efficient and IEX Cloud will charge you less credits compared to single url requests\n",
        "def get_historical_batch_urls(tickers, date_length, IEX_TOKEN=IEX_TOKEN):\n",
        "  '''Returns list of partitioned batch urls for api requests'''\n",
        "  historical_batch_urls = []\n",
        "  ticker_batches = [] #To have a recording of tickers in each batch\n",
        "\n",
        "  for ticker_partition in _partition(tickers): #Partitioned for 50 tickers in each batch url \n",
        "    ticker_batches.append(ticker_partition)\n",
        "    ticker_partition = \",\".join(ticker_partition)\n",
        "    #Batch Url should be changed to the respective sandbox mode url if you are testing if it works, you need to use a sandbox url\n",
        "    batch_url = (f\"https://cloud.iexapis.com/stable/stock/market/batch?symbols=\"\n",
        "                + f\"{ticker_partition}&types=chart&range={date_length}&token={IEX_TOKEN}\")\n",
        "    historical_batch_urls.append(batch_url)\n",
        "  return historical_batch_urls, ticker_batches\n",
        "\n",
        "def _partition(tickers, partition_size=50):\n",
        "  partitioned_tickers = []\n",
        "  for i in range(0, len(tickers), partition_size):\n",
        "    partitioned_tickers.append(tickers[i:i+partition_size])\n",
        "  return partitioned_tickers\n",
        "\n",
        "date_length = '15Y' #Max Data Length for IEX is 15 years\n",
        "historical_batch_urls, ticker_batches = get_historical_batch_urls(tickers, date_length)\n",
        "\n",
        "print('BATCH URL HERE')  #print(historical_batch_urls) #Can click the printed url batches to check, but will cost credits if not in sandbox\n",
        "print('We have {} batch urls that need to be downloaded'.format(len(historical_batch_urls)))"
      ],
      "metadata": {
        "id": "PMIyeBppRCOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c850400b-a05a-4a54-931b-51e0e0bed63c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH URL HERE\n",
            "We have 5 batch urls that need to be downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Segregate Batch URLS for easier consumption and slowly move the sliding window\n",
        "#Similar to what we did when downloading our Yahoo Finance Historicals\n",
        "#Reminder to delete API Key\n",
        "\n",
        "sliced_historical_batch_urls = historical_batch_urls[:5] #I am just going to do it all\n",
        "sliced_ticker_batches =  ticker_batches[:5] #Need this for analysis later, slice it the same as above for tracking purposes\n",
        "\n",
        "print(sliced_historical_batch_urls[0]) #Check the url, you can click it, but it will cost credits if not in Sandbox Mode"
      ],
      "metadata": {
        "id": "uyqZ_TyAREPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5e2c9d-a934-4b92-bb61-c7ef97263a2d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://cloud.iexapis.com/stable/stock/market/batch?symbols=AABA,ABI,ABKFQ,ACAS,ACS,AGN,AKS,ALXN,ANRZQ,APC,APCC,APOL,ARG,ASN,AT,AV,AVP,AW,AYE,BBT,BCR,BDK,BF.B,BHGE,BJS,BMC,BMET,BNI,BRCM,BRK.B,BSC,BTUUQ,BXLT,CAM,CBS,CBSS,CCE,CCTYQ,CEG,CELG,CEPH,CFC,CFN,CITGQ,CMCSK,CMVT,COG,COV,CPGX,CTL&types=chart&range=15Y&token=SOME_IEX_TOKEN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Request IEX Historicals from IEX Cloud and prepare to save them later as hdf5\n",
        "def get_iex_historicals(hist_batch_urls):\n",
        "  historicals = dict()\n",
        "  key_error_log = []\n",
        "  for hist_batch_url in hist_batch_urls:\n",
        "    try:\n",
        "      hist_response = requests.get(hist_batch_url)\n",
        "      hist_response.raise_for_status()\n",
        "      hist_response = hist_response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print('Stopped at batch url: {}'.format(hist_batch_url))\n",
        "      print('Status Code: {}'.format(hist_response.status_code))\n",
        "      raise SystemExit(e)\n",
        "    for ticker in hist_response:\n",
        "      ticker_hist = list()\n",
        "      total_amount_of_days = len(hist_response[ticker]['chart'])\n",
        "      for day in range(0, total_amount_of_days):\n",
        "        current_date = hist_response[ticker]['chart'][day]['date']\n",
        "        current_timestamp = dt.datetime.strptime(current_date,\"%Y-%m-%d\").timestamp() #Change to timestamp to save as hdf5\n",
        "        try:\n",
        "          ticker_hist.append([current_timestamp,\n",
        "                            hist_response[ticker]['chart'][day]['fOpen'], #The 'f' in front of the OHLC names hash for the adjusted prices\n",
        "                            hist_response[ticker]['chart'][day]['fHigh'], #fHigh is missing lol?\n",
        "                            hist_response[ticker]['chart'][day]['fLow'],\n",
        "                            hist_response[ticker]['chart'][day]['fClose'],\n",
        "                            hist_response[ticker]['chart'][day]['fVolume']])\n",
        "          historicals[ticker] = ticker_hist\n",
        "        except KeyError as e:\n",
        "          print(\"Key Error with {} at {} for {}\".format(current_date, ticker, e))\n",
        "          key_error_log.append([ticker, current_date, e])\n",
        "      print('Finished downloading {}'.format(ticker))\n",
        "  return historicals, key_error_log\n",
        "\n",
        "historicals, key_error_log = get_iex_historicals(sliced_historical_batch_urls)"
      ],
      "metadata": {
        "id": "83pA3-i1RFJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check your key errors, this occurs if a data point is missing; the entire OHLC for that day will not get added\n",
        "print(key_error_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECyEdddWHraB",
        "outputId": "75a76600-a82e-4cfa-f64c-149a85da72fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'VAR': ['2011-01-27', KeyError('fHigh')], 'EQ': ['2019-08-12', KeyError('fOpen')], 'PLL': ['2019-08-12', KeyError('fOpen')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter out historicals that didn't have data from IEX, to prevent saving empty hdf5 arrays\n",
        "#Two things to filter out here, historicals dict have ticker key but empty array or ticker missing from historical dict\n",
        "\n",
        "def filter_out_historicals_missing_from_iex(historicals, sliced_ticker_batches):\n",
        "  flat_ticker_batches = chain.from_iterable(sliced_ticker_batches) #Flatten it for a for loop\n",
        "  \n",
        "  missing_iex_tickers = [ticker \n",
        "                         for ticker in flat_ticker_batches\n",
        "                         if ticker not in list(historicals.keys())] #Check for if tickers are missing\n",
        "  \n",
        "  empty_historical_tickers =  [ticker\n",
        "                              for ticker, historicals in historicals.items()\n",
        "                              if not historicals]\n",
        "  return missing_iex_tickers, empty_historical_tickers\n",
        "\n",
        "missing_iex_tickers, empty_historical_tickers = filter_out_historicals_missing_from_iex(historicals, sliced_ticker_batches)\n",
        "print('We have {} missing tickers in this batch'.format(len(missing_iex_tickers)\n",
        "                                                        + len(empty_historical_tickers)))\n",
        "print(missing_iex_tickers[:10]) #We can take a peek out of our missing tickers here"
      ],
      "metadata": {
        "id": "HejqvPZjGeHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b01acbd3-aed7-4598-c6dc-6692a7295558"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 160 missing tickers in this batch\n",
            "['AABA', 'ABI', 'ABKFQ', 'ACAS', 'ACS', 'AGN', 'AKS', 'ANRZQ', 'APC', 'APCC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's add the missing_iex_tickers as empty lists to the historicals for data comparison\n",
        "def add_missing_iex_tickers_to_historicals(historicals, missing_iex_tickers):\n",
        "  for ticker in missing_iex_tickers:\n",
        "    historicals[ticker] = []\n",
        "  return historicals\n",
        "\n",
        "historicals = add_missing_iex_tickers_to_historicals(historicals, missing_iex_tickers)"
      ],
      "metadata": {
        "id": "S9vmhcn6C9n1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see what data we are still missing between our missing data and the actual data\n",
        "#You have to include missing_iex_tickers here as it was not included in our historicals Ok double check this\n",
        "\n",
        "def check_data_that_is_still_missing_from_batches(historicals, \n",
        "                                                  missing_iex_tickers,\n",
        "                                                  full_missing_tickers_and_dates):\n",
        "  data_still_missing = {ticker: np.setdiff1d(full_missing_tickers_and_dates[ticker], historicals[ticker])\n",
        "                       for ticker in historicals}\n",
        "  return data_still_missing\n",
        "\n",
        "def convert_timestamps_to_datetimes(data_still_missing):\n",
        "    data_still_missing = {ticker:\n",
        "                                  [dt.date.fromtimestamp(missing_date)\n",
        "                                  for missing_date in missing_dates]\n",
        "                          for ticker, missing_dates in data_still_missing.items()}\n",
        "    return data_still_missing\n",
        "\n",
        "data_still_missing = check_data_that_is_still_missing_from_batches(historicals, \n",
        "                                                                   missing_iex_tickers,\n",
        "                                                                   full_missing_tickers_and_dates)\n",
        "#converted_data_still_missing = convert_timestamps_to_datetimes(data_still_missing) #Can convert timestamps to dates to easily see the difference"
      ],
      "metadata": {
        "id": "b3Wxx_r_xcoE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute difference and see how much of the data we filled\n",
        "def calculate_data_reductions_from_iex(historicals,\n",
        "                                      data_still_missing,\n",
        "                                      full_missing_tickers_and_dates):\n",
        "\n",
        "  amount_still_missing = {ticker: len(data_still_missing[ticker])\n",
        "                          for ticker in historicals}\n",
        "  total_still_missing = sum(amount_still_missing.values())\n",
        "\n",
        "  amount_data_reductions = {ticker: len(full_missing_tickers_and_dates[ticker]) - len(data_still_missing[ticker])\n",
        "                          for ticker in historicals}\n",
        "  total_reductions = sum(amount_data_reductions.values())\n",
        "  return (amount_still_missing, total_still_missing, amount_data_reductions, total_reductions)\n",
        "\n",
        "(amount_still_missing, total_still_missing, \n",
        " amount_data_reductions, total_reductions) = calculate_data_reductions_from_iex(historicals, \n",
        "                                                                                data_still_missing, \n",
        "                                                                                full_missing_tickers_and_dates)\n",
        "total_inital_missing_data = total_still_missing + total_reductions\n",
        "\n",
        "print(\"Amount of data still missing {}\".format(total_still_missing))\n",
        "print(\"Amount of missing data we filled {}\".format(total_reductions))\n",
        "print(\"Of the {} total inital missing data for these batches, we filled {:.2%} for these batches\".format(total_inital_missing_data,\n",
        "                                                                                                         total_reductions/total_inital_missing_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD35f9xd6G1H",
        "outputId": "c0cf0ed7-8765-444a-9a4b-3c9798f80ef1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of data still missing 290871\n",
            "Amount of missing data we filled 36913\n",
            "Of the 327784 total inital missing data for these batches, we filled 11.26% for these batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_remaining_missing_tickers(data_still_missing):\n",
        "  remaining_missing_tickers = []\n",
        "  filled_tickers = []\n",
        "  for ticker, dates in data_still_missing.items():\n",
        "    if dates.size == 0:\n",
        "      filled_tickers.append(ticker)\n",
        "    else:\n",
        "       remaining_missing_tickers.append(ticker)\n",
        "  return remaining_missing_tickers, filled_tickers\n",
        "\n",
        "remaining_missing_tickers, filled_tickers = check_remaining_missing_tickers(data_still_missing)\n",
        "print(\"Amount of remaining missing tickers: {}\".format(len(remaining_missing_tickers)))\n",
        "print(\"Amount of missing tickers filled by IEX Cloud: {}\".format(len(filled_tickers)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cPnbLaHKEuX",
        "outputId": "dc74a504-0ef6-46b9-d120-221cee7b2485"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of remaining missing tickers: 228\n",
            "Amount of missing tickers filled by IEX Cloud: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In summary we got rid of around 10% of the missing data and 5 full tickers from our missing data\n",
        "#This is not bad, considering you will most likely have to use manual research to complete the rest of the missing tickers\n",
        "#From here I suggest looking at the data to see it matches the YF data and creating pipelines be able to quickly compare them\n",
        "#Additionally, we can understand that at this point manual research will benefit us a lot more than just pulling data from different online datasets"
      ],
      "metadata": {
        "id": "uGabOh0GQHyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Possible Next Steps"
      ],
      "metadata": {
        "id": "dk8IOgXIND4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop the filled_tickers from the data_still_missing dict\n",
        "#Then save the data_still_missing to a logs folder as a json\n",
        "\n",
        "def format_data_still_missing_for_json(data_still_missing):\n",
        "  formatted_data_still_missing = dict()\n",
        "\n",
        "  #Remove filled tickers as they are empty here (not missing anymore)\n",
        "  for ticker in filled_tickers:\n",
        "    data_still_missing.pop(ticker)\n",
        "\n",
        "   #Need ndarray as list for json\n",
        "  for ticker, missing_dates in data_still_missing.items():\n",
        "    formatted_data_still_missing[ticker] = missing_dates.tolist()\n",
        "  return  formatted_data_still_missing\n",
        "\n",
        "save_filepath = '/p3outputs/data_still_missing_after_iex.json'\n",
        "formatted_data_still_missing = format_data_still_missing_for_json(data_still_missing)\n",
        "\n",
        "with open(save_filepath, 'w', encoding='utf-8') as f:\n",
        "  json.dump(formatted_data_still_missing, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print('Data still missing has been saved as a json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InLbQP8mMKpB",
        "outputId": "d15eebc6-8b73-495b-8dd1-98477ec7ff00"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data still missing has been saved as a json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You will also need to one day merge the historicals from IEX to YF\n",
        "#I suggest saving three seperate databases one for YF, IEX, and a combined one of YF and IEX\n",
        "#This will reduce a lot of re-downloading of historicals if something goes bad during the merge\n",
        "#The functions I use to merge the datasets are as follows for reference\n",
        "#This does not take into account data that is wrong in the historicals, it is a naive merge\n",
        "#You will need to use a pipeline to check if data is consistent with your other databases\n",
        "\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "def merge_historicals(yf_historicals, iex_historicals):\n",
        "  historicals = dict()\n",
        "  all_tickers = set(yf_historicals.keys()) + set(iex_historicals.keys())\n",
        "  for ticker in all_tickers:\n",
        "    historicals[ticker] = pd.concat([yf_historicals[ticker], iex_historicals[ticker]])\n",
        "    historicals[ticker] = historicals[ticker].groupby(historicals[ticker].index).first().sort_index()\n",
        "  return historicals\n",
        "\n",
        "def test_for_ordinance(historicals):\n",
        "  '''\n",
        "  Description:\n",
        "    Double check that the sort and merge worked\n",
        "\n",
        "  Returns:\n",
        "    - True if all the data is in chronological order\n",
        "    - False if the data is not in chronological order\n",
        "  '''\n",
        "  ordinance = True\n",
        "  for ticker in historicals:\n",
        "    current_date = dt.date.min\n",
        "    for date in historicals[ticker].index:\n",
        "      if current_date > date or current_date == date:\n",
        "        print('Error with ticker {} on {}'.format(ticker, date))\n",
        "        ordinance = False\n",
        "        return ordinance\n",
        "      else:\n",
        "        current_date = date\n",
        "  return ordinance"
      ],
      "metadata": {
        "id": "oN4dzJcrPYE2"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#From here you are free to save these downloaded historicals\n",
        "#But again I suggest creating a complete IEX database and then merging the two on a seperate database\n",
        "#This way you can check that the data matches up in the pipeline and complete any safety checks without overwriting the seperate bases"
      ],
      "metadata": {
        "id": "Leq6IfejDr30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
