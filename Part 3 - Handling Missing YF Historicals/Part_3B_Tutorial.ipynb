{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part_3B_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1n2V_vVQ0En",
        "outputId": "ab125604-9cb9-4bbe-cfbb-7ec8cf26a37c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Imported Libraries\n"
          ]
        }
      ],
      "source": [
        "# We saw that we had a lot of missing data in Part 3A, in this section we will try to remedy that with another dataset.\n",
        "# This dataset comes from IEX Cloud. For 15Yr data you will need a subcription which is around 10USD per month.\n",
        "# It comes with realtime data, other financial/valuation metrics, and some level of customer support.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from itertools import chain\n",
        "\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from p3Binputs.apitokens import IEX_TOKEN  # Import your IEX_TOKEN.\n",
        "                                           # If you want to try this for free you can use IEX Sandbox Mode and a Sandbox IEX Token.\n",
        "print(\"Imported Libraries\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can decide if you want too create two full SP500 databases and average or merge your results for the most accurate OHLC.\n",
        "# Or you can just download only the historicals you are missing here. For now, I will download only the historicals we are missing.\n",
        "# But I suggest trying three databases, if you have storage for it. One for YF historicals, one for IEX, and one for the merged or average results.\n",
        "\n",
        "filepath = 'p3Binputs/full_missing_tickers_and_dates'\n",
        "\n",
        "with open(filepath, 'r') as f:\n",
        "  yf_missing_tickers_and_dates = json.load(f)\n",
        "\n",
        "tickers = list(yf_missing_tickers_and_dates.keys())\n",
        "print(tickers[:5])  # Take a look at the first 5 tickers we will request from IEX Cloud."
      ],
      "metadata": {
        "id": "aum8aCceQ-uq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6600024a-6152-4e2d-dc70-242bc73e8bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AABA', 'ABI', 'ABKFQ', 'ACAS', 'ACS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can make any changes to the tickers list here.\n",
        "# For example I know the Sears tickers was spelled wrong from Part 3A, it should be $SHLDQ not $SHLD.\n",
        "\n",
        "tickers.remove('SHLD')\n",
        "tickers.append('SHLDQ')\n",
        "yf_missing_tickers_and_dates['SHLDQ'] = yf_missing_tickers_and_dates.pop('SHLD') "
      ],
      "metadata": {
        "id": "tGN0slUoBVL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Batch Request Urls, batch urls are more efficient and IEX Cloud will charge you less credits compared to single url requests.\n",
        "\n",
        "def generate_iex_historical_batch_urls(tickers, date_length, partition_size=50, IEX_TOKEN=IEX_TOKEN):\n",
        "  '''Generates historical batch urls for IEX Cloud.\n",
        "  \n",
        "  Args:\n",
        "    tickers: list of tickers as strings.\n",
        "    date_length: string specifying how much data will be downloaded\n",
        "    partition_size: integer specifying how many tickers will be downloaded\n",
        "                    in each batch url. Max is 100 for IEX Cloud. Defaults 50.\n",
        "    IEX_TOKEN: string of your IEX TOKEN. Defaults to the imported IEX Token\n",
        "  \n",
        "  Returns:\n",
        "    historical_batch_urls: list of historical batch urls with specified partition_size\n",
        "    ticker_batches: list of lists with each list denoting the tickers in each \n",
        "                    generated batch url. Indices match the historical_batch_urls indices.\n",
        "  '''\n",
        "\n",
        "  historical_batch_urls = []\n",
        "  ticker_batches = []\n",
        "\n",
        "  for ticker_partition in partition(tickers, partition_size):\n",
        "    ticker_batches.append(ticker_partition)\n",
        "    ticker_partition = \",\".join(ticker_partition)\n",
        "    # The batch url should be changed to the respective sandbox mode url if you want to test if it works first.\n",
        "    batch_url = (f\"https://cloud.iexapis.com/stable/stock/market/batch?symbols=\"\n",
        "                + f\"{ticker_partition}&types=chart&range={date_length}&token={IEX_TOKEN}\")\n",
        "    historical_batch_urls.append(batch_url)\n",
        "  return historical_batch_urls, ticker_batches\n",
        "\n",
        "def partition(tickers, partition_size):\n",
        "  '''Partitions tickers into a list of lists with specified partition size.'''\n",
        "  partitioned_tickers = []\n",
        "  for i in range(0, len(tickers), partition_size):\n",
        "    partitioned_tickers.append(tickers[i:i+partition_size])\n",
        "  return partitioned_tickers\n",
        "\n",
        "date_length = '15Y'  # Max Data Length for IEX is 15 years.\n",
        "historical_batch_urls, ticker_batches = generate_iex_historical_batch_urls(tickers, date_length)\n",
        "\n",
        "print('BATCH URL HERE')   # print(historical_batch_urls)  # The printed url can be opended to verify the data, \n",
        "                                                          # but this will cost credits if you are not in sandbox mode.\n",
        "print(f'We have {len(historical_batch_urls)} batch urls that need to be downloaded')"
      ],
      "metadata": {
        "id": "PMIyeBppRCOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c850400b-a05a-4a54-931b-51e0e0bed63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH URL HERE\n",
            "We have 5 batch urls that need to be downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Segregate Batch URLS for easier consumption and slowly move the sliding window.\n",
        "# Similar to what we did when downloading our Yahoo Finance Historicals.\n",
        "\n",
        "sliced_historical_batch_urls = historical_batch_urls[:5]  # I will download them all at once.\n",
        "sliced_ticker_batches =  ticker_batches[:5]  # Will need data for analysis later, slice it at the same index above.\n",
        "print(sliced_historical_batch_urls[0])  # Can open the url to verify the data, but it will cost credits if you are not in sandbox mode."
      ],
      "metadata": {
        "id": "uyqZ_TyAREPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5e2c9d-a934-4b92-bb61-c7ef97263a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://cloud.iexapis.com/stable/stock/market/batch?symbols=AABA,ABI,ABKFQ,ACAS,ACS,AGN,AKS,ALXN,ANRZQ,APC,APCC,APOL,ARG,ASN,AT,AV,AVP,AW,AYE,BBT,BCR,BDK,BF.B,BHGE,BJS,BMC,BMET,BNI,BRCM,BRK.B,BSC,BTUUQ,BXLT,CAM,CBS,CBSS,CCE,CCTYQ,CEG,CELG,CEPH,CFC,CFN,CITGQ,CMCSK,CMVT,COG,COV,CPGX,CTL&types=chart&range=15Y&token=SOME_IEX_TOKEN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Request IEX Historicals from IEX Cloud and prepare to save them later as hdf5.\n",
        "\n",
        "def download_iex_historicals(batch_urls):\n",
        "  '''Downloads IEX historicals by making API requests to IEX Cloud.\n",
        "\n",
        "  Downloaded data is for the adjusted Open, High, Low, Close and Volume.\n",
        "  If no server response from IEX Cloud is recieved when the batch url requests\n",
        "  the data, the function will safely raise a SystemExit. A key error will be \n",
        "  logged if the ticker data for a date is missing.\n",
        "  \n",
        "  Args:\n",
        "    batch_urls: list of IEX batch urls.\n",
        "\n",
        "  Returns:\n",
        "    historicals: dict with tickers as keys and OHLC data as list of lists.\n",
        "                 Each list contains a date as a timestamp and the adjusted OHLCV data\n",
        "                 for that date.\n",
        "    key_error_log: list of lists of the key errors that occured. Each list contains\n",
        "                   which ticker that caused the key error and the date that it happened. \n",
        "\n",
        "  Raises:\n",
        "    RequestException: SystemExit, prints \"Stopped at batch url: {batch_url}\" and \"Status Code: {hist_response.status_code}\".\n",
        "    KeyError: Excepted, logs error into key_error_log.\n",
        "  '''\n",
        "\n",
        "  historicals = dict()\n",
        "  key_error_log = []\n",
        "\n",
        "  for batch_url in batch_urls:\n",
        "    try:\n",
        "      hist_response = requests.get(batch_url)\n",
        "      hist_response.raise_for_status()\n",
        "      hist_response = hist_response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f'Stopped at batch url: {batch_url}')\n",
        "      print(f'Status Code: {hist_response.status_code}')\n",
        "      raise SystemExit(e)\n",
        "    for ticker in hist_response:\n",
        "      ticker_hist = list()\n",
        "      total_amount_of_days = len(hist_response[ticker]['chart'])\n",
        "      for day in range(0, total_amount_of_days):\n",
        "        current_date = hist_response[ticker]['chart'][day]['date']\n",
        "        current_timestamp = dt.datetime.strptime(current_date,\"%Y-%m-%d\").timestamp()  # Change string date to timestamp to save as in hdf5 format.\n",
        "        try:\n",
        "          ticker_hist.append([current_timestamp,\n",
        "                              hist_response[ticker]['chart'][day]['fOpen'],  # As per IEX Cloud documentation the 'f' in front of\n",
        "                              hist_response[ticker]['chart'][day]['fHigh'],  # the OHLCV names specify for the adjusted OHLCV values.\n",
        "                              hist_response[ticker]['chart'][day]['fLow'],\n",
        "                              hist_response[ticker]['chart'][day]['fClose'],\n",
        "                              hist_response[ticker]['chart'][day]['fVolume']])\n",
        "          historicals[ticker] = ticker_hist\n",
        "        except KeyError as e:\n",
        "          print(f\"Key Error with {current_date} at {ticker} for {e}\")\n",
        "          key_error_log.append([ticker, current_date, e])\n",
        "      print(f'Finished downloading {ticker}')\n",
        "  return historicals, key_error_log\n",
        "\n",
        "historicals, key_error_log = download_iex_historicals(sliced_historical_batch_urls)"
      ],
      "metadata": {
        "id": "83pA3-i1RFJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your key errors, this occurs if a data point is missing; the entire OHLC for that day will not be added.\n",
        "\n",
        "print(key_error_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECyEdddWHraB",
        "outputId": "75a76600-a82e-4cfa-f64c-149a85da72fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'VAR': ['2011-01-27', KeyError('fHigh')], 'EQ': ['2019-08-12', KeyError('fOpen')], 'PLL': ['2019-08-12', KeyError('fOpen')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out historicals that didn't have data from IEX to prevent saving empty hdf5 arrays.\n",
        "# Two things to filter out here, the ticker is missing from the historicals dictionary or \n",
        "# the historicals dictionary has the ticker key but is paired with an empty array. \n",
        "\n",
        "def collect_tickers_not_found_on_iex(historicals, ticker_batches):\n",
        "  '''Collects tickers that were not avaliable on IEX Cloud.\n",
        "\n",
        "  There are two ways that the tickers are not found on IEX. The first \n",
        "  is that the ticker is missing from the historicals dict.The second is \n",
        "  that the historicals  dict has the ticker key but is paired with an empty \n",
        "  array. Both ways will be checked for and returned.  \n",
        "\n",
        "  Args:\n",
        "    historicals: dict with tickers as keys and OHLC data as a list of lists.\n",
        "    ticker_batches: list of lists with each list denoting the tickers in each \n",
        "                    generated batch url.\n",
        "\n",
        "  Returns:\n",
        "    missing_iex_tickers: list of tickers that are missing from the historical dict\n",
        "    empty_historical_tickers: list of tickers that are present in the historical dict,\n",
        "                              but are paired as empty arrays.\n",
        "  '''\n",
        "\n",
        "  flat_ticker_batches = chain.from_iterable(ticker_batches)  # Flatten the list of lists.\n",
        "  \n",
        "  # Check if the ticker keys are missing.\n",
        "  missing_iex_tickers = [ticker \n",
        "                         for ticker in flat_ticker_batches\n",
        "                         if ticker not in list(historicals.keys())]  \n",
        "  \n",
        "  # Check if the ticker keys are paired with empty arrays.\n",
        "  empty_historical_tickers =  [ticker\n",
        "                              for ticker, historicals in historicals.items()\n",
        "                              if not historicals]\n",
        "  return missing_iex_tickers, empty_historical_tickers\n",
        "\n",
        "missing_iex_tickers, empty_historical_tickers = collect_tickers_not_found_on_iex(historicals, sliced_ticker_batches)\n",
        "print(f'We have {len(missing_iex_tickers) + len(empty_historical_tickers)} missing tickers in this batch')\n",
        "print(missing_iex_tickers[:10])  # We can take a peek out of our missing tickers here."
      ],
      "metadata": {
        "id": "HejqvPZjGeHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b01acbd3-aed7-4598-c6dc-6692a7295558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 160 missing tickers in this batch\n",
            "['AABA', 'ABI', 'ABKFQ', 'ACAS', 'ACS', 'AGN', 'AKS', 'ANRZQ', 'APC', 'APCC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add the missing_iex_tickers as empty lists to the historicals for data comparison later.\n",
        "\n",
        "def add_missing_iex_tickers_to_historicals(historicals, missing_iex_tickers):\n",
        "  '''Adds missing IEX tickers to historicals as empty lists.'''\n",
        "  for ticker in missing_iex_tickers:\n",
        "    historicals[ticker] = []\n",
        "  return historicals\n",
        "\n",
        "historicals = add_missing_iex_tickers_to_historicals(historicals, missing_iex_tickers)"
      ],
      "metadata": {
        "id": "S9vmhcn6C9n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see what data we are still missing after both our Yahoo Finance and IEX Cloud data downloads.\n",
        "\n",
        "def collect_data_that_is_still_missing(historicals, yf_missing_tickers_and_dates):\n",
        "  '''Collects the tickers and dates that are still missing after the IEX download and YF download.'''\n",
        "  data_still_missing = {ticker: np.setdiff1d(yf_missing_tickers_and_dates[ticker], historicals[ticker])\n",
        "                       for ticker in historicals}\n",
        "  return data_still_missing\n",
        "\n",
        "def convert_timestamps_to_datetimes(data_still_missing):\n",
        "  '''Converts timestamps to datetimes.'''\n",
        "  data_still_missing = {ticker:\n",
        "                               [dt.date.fromtimestamp(missing_date)\n",
        "                               for missing_date in missing_dates]\n",
        "                       for ticker, missing_dates in data_still_missing.items()}\n",
        "  return data_still_missing\n",
        "\n",
        "data_still_missing = collect_data_that_is_still_missing(historicals, yf_missing_tickers_and_dates)\n",
        "# converted_data_still_missing = convert_timestamps_to_datetimes(data_still_missing)  # Timestamps can be converted to datetimes to easily assess the differences."
      ],
      "metadata": {
        "id": "b3Wxx_r_xcoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute difference and see how much of the data we filled.\n",
        "\n",
        "def calculate_metrics_of_added_data_from_iex(historicals, data_still_missing, yf_missing_tickers_and_dates):\n",
        "  '''Calculates updated missing data metrics with the addition of the IEX data.\n",
        "  \n",
        "  Args:\n",
        "    historicals: dict with tickers as keys and OHLC data as a list of lists.\n",
        "    data_still_missing: dict with tickers as keys and their \n",
        "                        current missing dates as values after IEX data addition.\n",
        "    yf_missing_tickers_and_dates: dict with tickers as keys and their missing\n",
        "                                  datas as values before IEX data addition.\n",
        "\n",
        "  Returns:\n",
        "    tickers_and_amount_missing: dict with tickers as keys and integer values specifying\n",
        "                                how much data is still missing for the ticker.\n",
        "    total_dates_missing: integer as the total amount of data still missing.\n",
        "    tickers_and_amount_reduced: dict with tickers as keys and integer values specifying\n",
        "                               how much data was reduced by adding the IEX data. \n",
        "    total_reductions: integer as the amount of missing data that has been \n",
        "                      reduced by the IEX data.\n",
        "  '''\n",
        "\n",
        "  tickers_and_amount_missing = {ticker: len(data_still_missing[ticker])\n",
        "                              for ticker in historicals}\n",
        "  total_dates_missing = sum(tickers_and_amount_missing.values())\n",
        "\n",
        "  tickers_and_amount_reduced = {ticker: len(yf_missing_tickers_and_dates[ticker]) - len(data_still_missing[ticker])\n",
        "                              for ticker in historicals}\n",
        "  total_reductions = sum(tickers_and_amount_reduced.values())\n",
        "  return (tickers_and_amount_missing, total_dates_missing, tickers_and_amount_reduced, total_reductions)\n",
        "\n",
        "(tickers_and_amount_missing, total_dates_missing, \n",
        " tickers_and_amount_reduced, total_reductions) = calculate_metrics_of_added_data_from_iex(historicals, data_still_missing, yf_missing_tickers_and_dates)\n",
        "total_inital_missing_data = total_dates_missing + total_reductions\n",
        "\n",
        "print(f\"Amount of data still missing {total_dates_missing}\")\n",
        "print(f\"Amount of missing data we filled {total_reductions}\")\n",
        "print(f\"Of the {total_inital_missing_data} total inital missing data for these batches, we filled {(total_reductions/total_inital_missing_data):.2%} for these batches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD35f9xd6G1H",
        "outputId": "c0cf0ed7-8765-444a-9a4b-3c9798f80ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of data still missing 290871\n",
            "Amount of missing data we filled 36913\n",
            "Of the 327784 total inital missing data for these batches, we filled 11.26% for these batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many tickers have been filled by downloading data from IEX Cloud.\n",
        "\n",
        "def check_remaining_missing_tickers(data_still_missing):\n",
        "  '''Checks which tickers are still missing from the database.\n",
        "  \n",
        "  Args:\n",
        "    data_still_missing: dict with tickers as keys and their \n",
        "                        current missing dates as values after IEX data addition.\n",
        "  \n",
        "  Returns:\n",
        "    remaining_missing_tickers: list\n",
        "    filled_tickers: list\n",
        "  '''\n",
        "\n",
        "  remaining_missing_tickers = []\n",
        "  filled_tickers = []\n",
        "\n",
        "  for ticker, dates in data_still_missing.items():\n",
        "    if dates.size == 0:\n",
        "      filled_tickers.append(ticker)\n",
        "    else:\n",
        "       remaining_missing_tickers.append(ticker)\n",
        "  return remaining_missing_tickers, filled_tickers\n",
        "\n",
        "remaining_missing_tickers, filled_tickers = check_remaining_missing_tickers(data_still_missing)\n",
        "print(f\"Amount of remaining missing tickers: {len(remaining_missing_tickers)}\")\n",
        "print(f\"Amount of missing tickers filled by IEX Cloud: {len(filled_tickers)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cPnbLaHKEuX",
        "outputId": "dc74a504-0ef6-46b9-d120-221cee7b2485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of remaining missing tickers: 228\n",
            "Amount of missing tickers filled by IEX Cloud: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In summary we got rid of around 10% of the missing data and 5 full tickers from our missing data.\n",
        "# This is not bad, considering you will most likely have to use manual research to complete the rest of the missing tickers.\n",
        "# From here I suggest looking at the data to see it matches the YF data and creating pipelines to be able to quickly compare them.\n",
        "# Additionally, we can understand that at this point manual research will benefit us a lot more than just pulling data from different online datasets."
      ],
      "metadata": {
        "id": "uGabOh0GQHyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Possible Next Steps"
      ],
      "metadata": {
        "id": "dk8IOgXIND4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the filled_tickers from the data_still_missing dict,\n",
        "# then save the data_still_missing to a logs folder as a json.\n",
        "\n",
        "def format_data_still_missing_for_json(data_still_missing):\n",
        "  '''Formats data that is still missing to a json format.'''\n",
        "  formatted_data_still_missing = dict()\n",
        "\n",
        "  # Remove filled tickers as they are empty here (not missing anymore).\n",
        "  for ticker in filled_tickers:\n",
        "    data_still_missing.pop(ticker)\n",
        "\n",
        "  # Need ndarray as a list for json file saving.\n",
        "  for ticker, missing_dates in data_still_missing.items():\n",
        "    formatted_data_still_missing[ticker] = missing_dates.tolist()\n",
        "  return  formatted_data_still_missing\n",
        "\n",
        "save_filepath = '/p3outputs/data_still_missing_after_iex.json'\n",
        "formatted_data_still_missing = format_data_still_missing_for_json(data_still_missing)\n",
        "\n",
        "with open(save_filepath, 'w', encoding='utf-8') as f:\n",
        "  json.dump(formatted_data_still_missing, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print('Data still missing has been saved as a json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InLbQP8mMKpB",
        "outputId": "d15eebc6-8b73-495b-8dd1-98477ec7ff00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data still missing has been saved as a json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You may also need to merge the historicals from IEX and YF.\n",
        "# I suggest saving three seperate databases one for YF, IEX, and a combined one for YF and IEX.\n",
        "# This will reduce a lot of re-downloading of historicals if something goes bad during the merge.\n",
        "# The functions I use to merge the datasets are as follows for reference.\n",
        "# This does not take into account data that is wrong in the historicals, it is a naive merge.\n",
        "# You will need to use a pipeline to check if the data is consistent with your other databases.\n",
        "\n",
        "def merge_historicals(yf_historicals, iex_historicals):\n",
        "  '''Merges given historicals.\n",
        "\n",
        "  Historicals are concated with each other and same or overlapping dates \n",
        "  are removed. The merged dataframe is then sorted by ascending dates \n",
        "  from past to present.\n",
        "  \n",
        "  Args:\n",
        "    yf_historicals: dict with tickers as keys and OHLC data as values.\n",
        "                    Each OHLC data is given as a pandas dataframe. \n",
        "    iex_historicals: dict with tickers as keys and OHLC data as values.\n",
        "                     Each OHLC data is given as a pandas dataframe. \n",
        "  \n",
        "  Returns:\n",
        "    merged_historicals: dict with tickers as keys and OHLC data as a pandas dataframe.\n",
        "  '''\n",
        "\n",
        "  merged_historicals = dict()\n",
        "\n",
        "  all_tickers = set(yf_historicals.keys()) + set(iex_historicals.keys())\n",
        "  for ticker in all_tickers:\n",
        "    merged_historicals[ticker] = pd.concat([yf_historicals[ticker], iex_historicals[ticker]])\n",
        "    merged_historicals[ticker] = merged_historicals[ticker].groupby(merged_historicals[ticker].index).first().sort_index()\n",
        "  return merged_historicals\n",
        "\n",
        "def test_for_ordinance(historicals):\n",
        "  '''Tests that historicals are all in chronological order.\n",
        "\n",
        "  Args:\n",
        "    historicals: dict with tickers as keys and OHLC data as values.\n",
        "                 Each OHLC data is given as a pandas dataframe. \n",
        "\n",
        "  Returns:\n",
        "    ordinance: bool. Will be True if all the data is in chronological order\n",
        "               or False if the data is not in chronological order\n",
        "  '''\n",
        "\n",
        "  ordinance = True\n",
        "  for ticker in historicals:\n",
        "    current_date = dt.date.min\n",
        "    for date in historicals[ticker].index:\n",
        "      if current_date > date or current_date == date:\n",
        "        print(f'Error with ticker {ticker} on {date}')\n",
        "        ordinance = False\n",
        "        return ordinance\n",
        "      else:\n",
        "        current_date = date\n",
        "  return ordinance"
      ],
      "metadata": {
        "id": "oN4dzJcrPYE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From here you are free to save these downloaded historicals.\n",
        "# But I suggest creating a complete IEX database and then merging the two on a seperate database.\n",
        "# This way you can check that the data matches up in the pipeline and complete any safety checks without overwriting the seperate databases."
      ],
      "metadata": {
        "id": "Leq6IfejDr30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}